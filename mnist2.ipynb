{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn import metrics\n",
    "from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    N = layer_dims[0]\n",
    "    xavier = 1 / N\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters[l] = {\n",
    "            'W': np.random.randn(layer_dims[l], layer_dims[l-1]) * xavier,\n",
    "            'b': np.zeros((layer_dims[l], 1))\n",
    "        }\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_forward(A, W, b, activation):\n",
    "    Z = np.dot(W, A) + b\n",
    "    p_A = A\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        A, Z = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        A, Z = relu(Z)\n",
    "    \n",
    "    cache2 = {\n",
    "        'A': p_A,\n",
    "        'W': W,\n",
    "        'Z': Z\n",
    "    }\n",
    "\n",
    "    return A, cache2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    caches = {}\n",
    "    A = X\n",
    "    L = len(parameters) # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        params = parameters[l]\n",
    "        A, cache = linear_activation_forward(A_prev, params['W'], params['b'], 'relu')\n",
    "        caches[l-1] = cache\n",
    "            \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    params = parameters[L]\n",
    "    AL, cache, = linear_activation_forward(A, params['W'], params['b'], 'sigmoid')\n",
    "    caches[L-1] = cache\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(A, Y):\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    # Compute loss from aL and y.\n",
    "    cost = np.mean(np.sum(((Y * np.log(A)) + ((1 - Y) * (np.log(1 - A)))), axis=1, keepdims=True) / -m)\n",
    "#     cost = np.mean(np.sum(((-np.dot(Y,np.log(A).T) - np.dot(1-Y, np.log(1 - A).T)) / m), axis=1, keepdims=True))\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, cache['Z'])\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, cache['Z'])\n",
    "    \n",
    "    A_prev = cache['A']\n",
    "    W = cache['W']\n",
    "    m = A_prev.shape[1]\n",
    "    dW = (np.dot(dZ, A_prev.T)) / m\n",
    "    db = (np.sum(dZ, axis=1, keepdims=True)) / m\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[len(caches)-1]\n",
    "    dA, dW, db = linear_activation_backward(dAL, current_cache, 'sigmoid')\n",
    "    grads[L] = {\n",
    "        'dA': dA,\n",
    "        'dW': dW,\n",
    "        'db': db\n",
    "    }\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 2)], caches\". Outputs: \"grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        current_cache = caches[l]\n",
    "        dA, dW, db = linear_activation_backward(grads[l+2]['dA'], current_cache, 'relu')\n",
    "        grads[l+1] = {\n",
    "            'dA': dA,\n",
    "            'dW': dW,\n",
    "            'db': db\n",
    "        }\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters)\n",
    "\n",
    "    for l in range(L):\n",
    "        params = parameters[l+1]\n",
    "        parameters[l+1]['W'] = params['W'] - (learning_rate * grads[l+1]['dW'])\n",
    "        parameters[l+1]['b'] = params['b'] - (learning_rate * grads[l+1]['db'])\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
    "    np.random.seed(1)\n",
    "    costs = []\n",
    "    \n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        \n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL, Y)\n",
    "    \n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    " \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label:0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1121d05f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def to_one_hot(Y, num_items):\n",
    "    newY = np.zeros((0, num_items))\n",
    "    i = 0\n",
    "    for y in Y:\n",
    "        newy = np.zeros((1, num_items))\n",
    "        newy[0, y[0]] = 1\n",
    "        newY = np.append(newY, newy, axis=0)\n",
    "    return newY\n",
    "\n",
    "file_name = './input/train.csv'\n",
    "df = pd.read_csv(file_name, header = 0)\n",
    "\n",
    "original_headers = list(df.columns.values)\n",
    "numpy_array = df.as_matrix()\n",
    "\n",
    "numpy_array_random = numpy_array[:, :]\n",
    "# np.random.shuffle(numpy_array_random)\n",
    "\n",
    "num_items = 10\n",
    "Y_orig = numpy_array_random[:, 1:2]\n",
    "Y = to_one_hot(Y_orig, num_items).T\n",
    "X = numpy_array_random[:, 2:].T\n",
    "X_count, m = X.shape\n",
    "x_count = int(np.sqrt(X_count))\n",
    "\n",
    "X = (X - 128) / 128\n",
    "\n",
    "index = 5\n",
    "item = X[:, index]\n",
    "img_X = item.reshape((x_count, x_count))\n",
    "plt.imshow(img_X, shape=(x_count, x_count))\n",
    "print('label:' + str(Y_orig[index, 0]))\n",
    "\n",
    "trainSize = 25000\n",
    "devSize = (m - trainSize) // 2\n",
    "train_X, dev_X, test_X = X[:, :trainSize], X[:, trainSize:(trainSize + devSize)], X[:, (trainSize + devSize):]\n",
    "train_Y, dev_Y, test_Y = Y[:, :trainSize], Y[:, trainSize:(trainSize + devSize)], Y[:, (trainSize + devSize):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n"
     ]
    }
   ],
   "source": [
    "NN_shape = [X_count, 20, 15, num_items]\n",
    "parameters = L_layer_model(train_X, train_Y, NN_shape, learning_rate = 2.5, num_iterations = 5000, print_cost = True)\n",
    "\n",
    "# Cost after iteration 0: 0.693153\n",
    "# Cost after iteration 100: 0.321805\n",
    "# Cost after iteration 200: 0.318209\n",
    "# Cost after iteration 300: 0.324920\n",
    "# Cost after iteration 400: 0.324920\n",
    "# Cost after iteration 500: 0.324920\n",
    "# Cost after iteration 600: 0.324919"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pred_train = predict(train_X, train_Y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-217-42a904d2bf15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprobas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mpred_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-217-42a904d2bf15>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(X, y, parameters)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Forward propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mprobas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaches2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL_model_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mnewY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-215-c09c4fa07626>\u001b[0m in \u001b[0;36mL_model_forward\u001b[0;34m(X, parameters)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mA_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_activation_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mcaches2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "    \n",
    "    newY = np.zeros((1, y.shape[1]))\n",
    "    newProbas = np.zeros((1, probas.shape[1]))\n",
    "    \n",
    "    no_equal_cnt = 0\n",
    "    images_count = 0\n",
    "    for i in range(0, y.shape[1]):\n",
    "        newY[0, i] = np.argmax(y[:, i], axis=0)\n",
    "        newProbas[0, i] = np.argmax(probas[:, i], axis=0)\n",
    "        \n",
    "        if newY[0, i] != newProbas[0, i]:\n",
    "            no_equal_cnt += 1\n",
    "            \n",
    "            if images_count < 10:\n",
    "                print('>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "                item = X[:, i]\n",
    "                plt.figure()\n",
    "                plt.imshow(item.reshape((28, 28)), shape=(28, 28))\n",
    "                plt.show()\n",
    "                print('prediced label: ' + str(newProbas[0, i]) + ' || real label: ' + str(newY[0, i]))\n",
    "                images_count += 1\n",
    "    \n",
    "    print('')\n",
    "    print('not equal cnt: ' + str(no_equal_cnt))\n",
    "    print('')\n",
    "    print('')\n",
    "    print(metrics.classification_report(newProbas.T, newY.T))\n",
    "    return probas\n",
    "\n",
    "pred_dev = predict(dev_X, dev_Y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "c4HO0",
   "launcher_item_id": "lSYZM"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
